{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# llama 3.1b-8b MFTC"
      ],
      "metadata": {
        "id": "1iAQv-8TTgg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"fireworks-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFTCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['tweet_text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNk3FPYETgBn",
        "outputId": "6305a369-87f2-43d1-8659-14700c85305e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:36<00:00,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.23      0.27      0.25        11\n",
            "   Fairness/Cheating       0.57      0.25      0.35        16\n",
            "    Loyalty/Betrayal       0.08      0.17      0.11         6\n",
            "Authority/Subversion       0.22      0.15      0.18        13\n",
            "  Purity/Degradation       1.00      0.17      0.29         6\n",
            "           Non-Moral       0.60      0.06      0.10        53\n",
            "\n",
            "           micro avg       0.30      0.13      0.18       105\n",
            "           macro avg       0.45      0.18      0.21       105\n",
            "        weighted avg       0.50      0.13      0.18       105\n",
            "         samples avg       0.07      0.13      0.09       105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deepseek v3"
      ],
      "metadata": {
        "id": "RoEjiiIcaRip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPXVP1IYTdHJ",
        "outputId": "54702a59-c1d2-494b-e29c-ed4bf81ac9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:41<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00        11\n",
            "   Fairness/Cheating       0.50      0.12      0.20        16\n",
            "    Loyalty/Betrayal       0.33      0.17      0.22         6\n",
            "Authority/Subversion       0.00      0.00      0.00        13\n",
            "  Purity/Degradation       0.00      0.00      0.00         6\n",
            "           Non-Moral       0.62      0.09      0.16        53\n",
            "\n",
            "           micro avg       0.42      0.08      0.13       105\n",
            "           macro avg       0.24      0.06      0.10       105\n",
            "        weighted avg       0.41      0.08      0.13       105\n",
            "         samples avg       0.07      0.08      0.07       105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"fireworks-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFTCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"deepseek-ai/DeepSeek-V3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['tweet_text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# google gemma 3 -12b-it"
      ],
      "metadata": {
        "id": "VqUgAOk7cvza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"featherless-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFTCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"google/gemma-3-12b-it\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['tweet_text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohgf4OgqazTS",
        "outputId": "6d5cbd7b-f81d-49b6-80c8-32d28c860ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.33      0.27      0.30        11\n",
            "   Fairness/Cheating       0.36      0.25      0.30        16\n",
            "    Loyalty/Betrayal       0.10      0.17      0.12         6\n",
            "Authority/Subversion       0.10      0.08      0.09        13\n",
            "  Purity/Degradation       0.17      0.17      0.17         6\n",
            "           Non-Moral       0.50      0.04      0.07        53\n",
            "\n",
            "           micro avg       0.24      0.11      0.15       105\n",
            "           macro avg       0.26      0.16      0.17       105\n",
            "        weighted avg       0.37      0.11      0.14       105\n",
            "         samples avg       0.06      0.11      0.07       105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Harm'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#mistral"
      ],
      "metadata": {
        "id": "3m58-knXdlxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"novita\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFTCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['tweet_text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWjRtOhydIuR",
        "outputId": "3a7d66df-a41d-424c-9a13-a890e81ea9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.13      0.27      0.18        11\n",
            "   Fairness/Cheating       0.80      0.25      0.38        16\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         6\n",
            "Authority/Subversion       0.25      0.15      0.19        13\n",
            "  Purity/Degradation       0.33      0.33      0.33         6\n",
            "           Non-Moral       0.88      0.13      0.23        53\n",
            "\n",
            "           micro avg       0.36      0.17      0.23       105\n",
            "           macro avg       0.40      0.19      0.22       105\n",
            "        weighted avg       0.63      0.17      0.24       105\n",
            "         samples avg       0.13      0.17      0.14       105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Hate Speech', 'Insult', 'Racism'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# qwen2.b-7b-instruct"
      ],
      "metadata": {
        "id": "ADlnqO2Te-qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"featherless-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFTCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['tweet_text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MebG5XxPdy61",
        "outputId": "d821a841-1d96-4cef-a1a8-88a1b55b46b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:39<00:00,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00        11\n",
            "   Fairness/Cheating       1.00      0.12      0.22        16\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         6\n",
            "Authority/Subversion       0.25      0.15      0.19        13\n",
            "  Purity/Degradation       0.17      0.17      0.17         6\n",
            "           Non-Moral       0.50      0.04      0.07        53\n",
            "\n",
            "           micro avg       0.22      0.07      0.10       105\n",
            "           macro avg       0.32      0.08      0.11       105\n",
            "        weighted avg       0.45      0.07      0.10       105\n",
            "         samples avg       0.06      0.07      0.06       105\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Harm', 'Loyalty'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFRC Zero shot"
      ],
      "metadata": {
        "id": "wAK2YXh-f4y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llama"
      ],
      "metadata": {
        "id": "pkuHdV4mjL94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"fireworks-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFRCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGQRcAGIfS34",
        "outputId": "6f410601-0db0-4d5d-f931-ea57608d342e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:38<00:00,  2.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00         4\n",
            "   Fairness/Cheating       0.08      0.14      0.11         7\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         3\n",
            "Authority/Subversion       0.00      0.00      0.00         0\n",
            "  Purity/Degradation       0.33      1.00      0.50         1\n",
            "           Non-Moral       1.00      0.06      0.12        63\n",
            "\n",
            "           micro avg       0.13      0.08      0.10        78\n",
            "           macro avg       0.24      0.20      0.12        78\n",
            "        weighted avg       0.82      0.08      0.11        78\n",
            "         samples avg       0.05      0.06      0.05        78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deepseek v3"
      ],
      "metadata": {
        "id": "2N7bCVDAkW04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"fireworks-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFRCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"deepseek-ai/DeepSeek-V3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iU2nNBdjYEE",
        "outputId": "65d2a999-1286-4615-d8b3-98a56e430134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:46<00:00,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00         4\n",
            "   Fairness/Cheating       0.00      0.00      0.00         7\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         3\n",
            "Authority/Subversion       0.00      0.00      0.00         0\n",
            "  Purity/Degradation       0.00      0.00      0.00         1\n",
            "           Non-Moral       0.89      0.13      0.22        63\n",
            "\n",
            "           micro avg       0.47      0.10      0.17        78\n",
            "           macro avg       0.15      0.02      0.04        78\n",
            "        weighted avg       0.72      0.10      0.18        78\n",
            "         samples avg       0.08      0.08      0.08        78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google gemma 3"
      ],
      "metadata": {
        "id": "ZIt7JwB9k-G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"featherless-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFRCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"google/gemma-3-12b-it\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDZ3zUg_kiOC",
        "outputId": "358e8200-4959-4d1f-c68a-eeb91f72111e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:17<00:00,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.17      0.25      0.20         4\n",
            "   Fairness/Cheating       0.18      0.43      0.25         7\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         3\n",
            "Authority/Subversion       0.00      0.00      0.00         0\n",
            "  Purity/Degradation       0.00      0.00      0.00         1\n",
            "           Non-Moral       1.00      0.11      0.20        63\n",
            "\n",
            "           micro avg       0.20      0.14      0.16        78\n",
            "           macro avg       0.22      0.13      0.11        78\n",
            "        weighted avg       0.83      0.14      0.19        78\n",
            "         samples avg       0.09      0.11      0.10        78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mistral"
      ],
      "metadata": {
        "id": "iroUdYCVluJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"novita\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFRCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHLFK4NBlvzp",
        "outputId": "21f85c8a-d88d-4ef2-ef93-a5b51e5d1019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:42<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00         4\n",
            "   Fairness/Cheating       0.00      0.00      0.00         7\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         3\n",
            "Authority/Subversion       0.00      0.00      0.00         0\n",
            "  Purity/Degradation       0.00      0.00      0.00         1\n",
            "           Non-Moral       1.00      0.02      0.03        63\n",
            "\n",
            "           micro avg       0.04      0.01      0.02        78\n",
            "           macro avg       0.17      0.00      0.01        78\n",
            "        weighted avg       0.81      0.01      0.03        78\n",
            "         samples avg       0.01      0.01      0.01        78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## qwen 2.5b"
      ],
      "metadata": {
        "id": "-h0UAoj6mB8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from huggingface_hub import InferenceClient\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Initialize Hugging Face Inference Client\n",
        "client = InferenceClient(\n",
        "    provider=\"featherless-ai\",\n",
        "    api_key=\"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\",  # Replace with your actual key\n",
        ")\n",
        "\n",
        "# Define the 6 moral classes used in your dataset\n",
        "moral_classes = [\n",
        "    \"Care/Harm\",\n",
        "    \"Fairness/Cheating\",\n",
        "    \"Loyalty/Betrayal\",\n",
        "    \"Authority/Subversion\",\n",
        "    \"Purity/Degradation\",\n",
        "    \"Non-Moral\"\n",
        "]\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/llm MFRCfoundations.csv\")\n",
        "\n",
        "# Extract true labels from the binary columns\n",
        "def extract_labels(row):\n",
        "    labels = [col for col in moral_classes if row[col] == 1]\n",
        "    return sorted(labels)\n",
        "\n",
        "df['labels'] = df.apply(extract_labels, axis=1)\n",
        "\n",
        "# Zero-shot classification using LLaMA-3\n",
        "def classify_text(text):\n",
        "    prompt = f\"\"\"\n",
        "Classify the following text into moral foundation categories from this list:\n",
        "{', '.join(moral_classes)}.Your response must\n",
        " contain just the list of the detected moral traits, do\n",
        " not add any additional word or introductions.\n",
        "Return the result strictly in this Python list format: [<list of detected moral traits>].\n",
        "If none apply, return ['Non-Moral'] only.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{text.strip()}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        response = completion.choices[0].message.content.strip()\n",
        "        detected = eval(response) if response.startswith(\"[\") else []\n",
        "        return sorted(set(detected))\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Get predictions\n",
        "df['predicted'] = [classify_text(text) for text in tqdm(df['text'])]\n",
        "\n",
        "# Flatten the multilabel data for evaluation\n",
        "mlb = MultiLabelBinarizer(classes=moral_classes)\n",
        "y_true = mlb.fit_transform(df['labels'])\n",
        "y_pred = mlb.transform(df['predicted'])\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred, target_names=moral_classes, zero_division=0)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFR1ahOsmHq6",
        "outputId": "5bba9691-0238-4d3c-c294-23bcad0d55dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:37<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "           Care/Harm       0.00      0.00      0.00         4\n",
            "   Fairness/Cheating       0.00      0.00      0.00         7\n",
            "    Loyalty/Betrayal       0.00      0.00      0.00         3\n",
            "Authority/Subversion       0.00      0.00      0.00         0\n",
            "  Purity/Degradation       0.17      1.00      0.29         1\n",
            "           Non-Moral       1.00      0.06      0.12        63\n",
            "\n",
            "           micro avg       0.20      0.06      0.10        78\n",
            "           macro avg       0.19      0.18      0.07        78\n",
            "        weighted avg       0.81      0.06      0.10        78\n",
            "         samples avg       0.05      0.05      0.05        78\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}